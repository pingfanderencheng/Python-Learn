{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 161.1: Setting up py.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "py.test is one of several [third party testing libraries](http://docs.pytest.org/en/latest/) that are available for Python. It can be installed using pip with"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Say we are testing an addition function in mycode.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a, b):\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We create a test file in test_code.py . The file must begin with test_ to be recognized as a testing file."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Running The Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Windows [版本 10.0.16299.309]\n",
      "(c) 2017 Microsoft Corporation。保留所有权利。\n",
      "\n",
      "E:\\MyFile\\Jupyter\\Python-Learn\\Chapter 161 py.test>py.test\n",
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.6.3, pytest-3.2.1, py-1.4.34, pluggy-0.4.0\n",
      "rootdir: E:\\MyFile\\Jupyter\\Python-Learn\\Chapter 161 py.test, inifile:\n",
      "collected 1 item\n",
      "\n",
      "test_code.py .\n",
      "\n",
      "========================== 1 passed in 0.03 seconds ===========================\n",
      "\n",
      "E:\\MyFile\\Jupyter\\Python-Learn\\Chapter 161 py.test>"
     ]
    }
   ],
   "source": [
    "%%cmd\n",
    "py.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 161.2: Intro to Test Fixtures"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "More complicated tests sometimes need to have things set up before you run the code you want to test. It is\n",
    "possible to do this in the test function itself, but then you end up with large test functions doing so much that it is\n",
    "difficult to tell where the setup stops and the test begins. You can also get a lot of duplicate setup code between\n",
    "your various test functions."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Our code file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stuff(object):\n",
    "    \n",
    "    def prep(self):\n",
    "        self.foo = 1\n",
    "        self.bar = 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Our test file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "from module import stuff\n",
    "    def test_foo_updates():\n",
    "        my_stuff = stuff.Stuff()\n",
    "        my_stuff.prep()\n",
    "        assert 1 == my_stuff.foo\n",
    "        my_stuff.foo = 30000\n",
    "        assert my_stuff.foo == 30000\n",
    "    def test_bar_updates():\n",
    "        my_stuff = stuff.Stuff()\n",
    "        my_stuff.prep()\n",
    "        assert 2 == my_stuff.bar\n",
    "        my_stuff.bar = 42\n",
    "        assert 42 == my_stuff.bar"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "These are pretty simple examples, but if our Stuff object needed a lot more setup, it would get unwieldy. We see\n",
    "that there is some duplicated code between our test cases, so let's refactor that into a separate function first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "from module import stuff\n",
    "    def get_prepped_stuff():\n",
    "        my_stuff = stuff.Stuff()\n",
    "        my_stuff.prep()\n",
    "        return my_stuff\n",
    "    def test_foo_updates():\n",
    "        my_stuff = get_prepped_stuff()\n",
    "        assert 1 == my_stuff.foo\n",
    "        my_stuff.foo = 30000\n",
    "        assert my_stuff.foo == 30000\n",
    "    def test_bar_updates():\n",
    "        my_stuff = get_prepped_stuff()\n",
    "        assert 2 == my_stuff.bar\n",
    "        my_stuff.bar = 42\n",
    "        assert 42 == my_stuff.bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**py.test fixtures to the rescue!**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Fixtures are much more powerful and flexible versions of test setup functions. They can do a lot more than we're\n",
    "leveraging here, but we'll take it one step at a time."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "First we change get_prepped_stuff to a fixture called prepped_stuff . You want to name your fixtures with nouns\n",
    "rather than verbs because of how the fixtures will end up being used in the test functions themselves later. The\n",
    "@pytest.fixture indicates that this specific function should be handled as a fixture rather than a regular function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture\n",
    "def prepped_stuff():\n",
    "    my_stuff = stuff.Stuff()\n",
    "    my_stuff.prep()\n",
    "    return my_stuff"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now we should update the test functions so that they use the fixture. This is done by adding a parameter to their\n",
    "definition that exactly matches the fixture name. When py.test executes, it will run the fixture before running the\n",
    "test, then pass the return value of the fixture into the test function through that parameter. (Note that fixtures\n",
    "don't need to return a value; they can do other setup things instead, like calling an external resource, arranging\n",
    "things on the filesystem, putting values in a database, whatever the tests need for setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_foo_updates(prepped_stuff):\n",
    "    my_stuff = prepped_stuff\n",
    "    assert 1 == my_stuff.foo\n",
    "    my_stuff.foo = 30000\n",
    "    assert my_stuff.foo == 30000\n",
    "def test_bar_updates(prepped_stuff):\n",
    "    my_stuff = prepped_stuff\n",
    "    assert 2 == my_stuff.bar\n",
    "    my_stuff.bar = 42\n",
    "    assert 42 == my_stuff.bar"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now you can see why we named it with a noun. but the my_stuff = prepped_stuff line is pretty much useless, so\n",
    "let's just use prepped_stuff directly instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_foo_updates(prepped_stuff):\n",
    "    assert 1 == prepped_stuff.foo\n",
    "    prepped_stuff.foo = 30000\n",
    "    assert prepped_stuff.foo == 30000\n",
    "def test_bar_updates(prepped_stuff):\n",
    "    assert 2 == prepped_stuff.bar\n",
    "    prepped_stuff.bar = 42\n",
    "    assert 42 == prepped_stuff.bar"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now we're using fixtures! We can go further by changing the scope of the fixture (so it only runs once per test\n",
    "module or test suite execution session instead of once per test function), building fixtures that use other fixtures,\n",
    "parametrizing the fixture (so that the fixture and all tests using that fixture are run multiple times, once for each\n",
    "parameter given to the fixture), fixtures that read values from the module that calls them... as mentioned earlier,\n",
    "fixtures have a lot more power and flexibility than a normal setup function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleaning up after the tests are done.**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We could add some code to call the clean up at the bottom of every test function, but fixtures provide a better way\n",
    "to do this. If you add a function to the fixture and register it as a finalizer, the code in the finalizer function will get\n",
    "called after the test using the fixture is done. If the scope of the fixture is larger than a single function (like module\n",
    "or session), the finalizer will be executed after all the tests in scope are completed, so after the module is done\n",
    "running or at the end of the entire test running session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture\n",
    "def prepped_stuff(request): # we need to pass in the request to use finalizers\n",
    "    my_stuff = stuff.Stuff()\n",
    "    my_stuff.prep()\n",
    "    def fin(): # finalizer function\n",
    "        # do all the cleanup here\n",
    "        my_stuff.finish()\n",
    "    request.addfinalizer(fin) # register fin() as a finalizer\n",
    "    # you can do more setup here if you really want to\n",
    "    return my_stuff"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Using the finalizer function inside a function can be a bit hard to understand at first glance, especially when you\n",
    "have more complicated fixtures. You can instead use a yield fixture to do the same thing with a more human\n",
    "readable execution flow. The only real difference is that instead of using return we use a yield at the part of the\n",
    "fixture where the setup is done and control should go to a test function, then add all the cleanup code after the\n",
    "yield . We also decorate it as a yield_fixture so that py.test knows how to handle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.yield_fixture\n",
    "def prepped_stuff(): # it doesn't need request now!\n",
    "    # do setup\n",
    "    my_stuff = stuff.Stuff()\n",
    "    my_stuff.prep()\n",
    "    # setup is done, pass control to the test functions\n",
    "    yield my_stuff\n",
    "    # do cleanup\n",
    "    my_stuff.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "And that concludes the Intro to Test Fixtures!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information, see the [official py.test fixture documentation](http://doc.pytest.org/en/latest/fixture.html) and the [official yield fixture documentation](http://doc.pytest.org/en/latest/yieldfixture.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 161.3: Failing Tests"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A failing test will provide helpful output as to what went wrong:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from module import code\n",
    "    def test_add__failing():\n",
    "        assert code.add(10, 11) == 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Windows [版本 10.0.16299.309]\n",
      "(c) 2017 Microsoft Corporation。保留所有权利。\n",
      "\n",
      "E:\\MyFile\\Jupyter\\Python-Learn\\Chapter 161 py.test>py.test\n",
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.6.3, pytest-3.2.1, py-1.4.34, pluggy-0.4.0\n",
      "rootdir: E:\\MyFile\\Jupyter\\Python-Learn\\Chapter 161 py.test, inifile:\n",
      "collected 4 items\n",
      "\n",
      "test_code.py .\n",
      "test_fail.py F\n",
      "test_stuff.py ..\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "______________________________ test_add__failing ______________________________\n",
      "\n",
      "    def test_add__failing():\n",
      ">       assert my_code.add(10, 11) == 33\n",
      "E       assert 21 == 33\n",
      "E        +  where 21 = <function add at 0x0000023E87886950>(10, 11)\n",
      "E        +    where <function add at 0x0000023E87886950> = my_code.add\n",
      "\n",
      "test_fail.py:3: AssertionError\n",
      "===================== 1 failed, 3 passed in 0.09 seconds ======================\n",
      "\n",
      "E:\\MyFile\\Jupyter\\Python-Learn\\Chapter 161 py.test>"
     ]
    }
   ],
   "source": [
    "%%cmd\n",
    "py.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
