{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "PLY is a pure-Python implementation of the popular compiler construction tools lex and yacc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 190.1: Getting Started with PLY"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To install PLY on your machine for python2/3, follow the steps outlined below:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Download the source code from here. 1.\n",
    "Unzip the downloaded zip file 2.\n",
    "Navigate into the unzipped ply-3.10 folder 3.\n",
    "Run the following command in your terminal: python setup.py install 4."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "If you completed all the above, you should now be able to use the PLY module. You can test it out by opening a\n",
    "python interpreter and typing import ply.lex "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: Do not use pip to install PLY, it will install a broken distribution on your machine.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 190.2: The \"Hello, World!\" of PLY - A Simple Calculator"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Let's demonstrate the power of PLY with a simple example: this program will take an arithmetic expression as a\n",
    "string input, and attempt to solve it."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Open up your favourite editor and copy the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "from ply import lex\n",
    "import ply.yacc as yacc\n",
    "tokens = (\n",
    "    'PLUS',\n",
    "    'MINUS',\n",
    "    'TIMES',\n",
    "    'DIV',\n",
    "    'LPAREN',\n",
    "    'RPAREN',\n",
    "    'NUMBER',\n",
    ")\n",
    "t_ignore = ' \\t'\n",
    "t_PLUS = r'\\+'\n",
    "t_MINUS = r'-'\n",
    "t_TIMES = r'\\*'\n",
    "t_DIV = r'/'\n",
    "t_LPAREN = r'\\('\n",
    "t_RPAREN = r'\\)'\n",
    "def t_NUMBER( t ) :\n",
    "    r'[0-9]+'\n",
    "    t.value = int( t.value )\n",
    "    return t\n",
    "def t_newline( t ):\n",
    "    r'\\n+'\n",
    "    t.lexer.lineno += len( t.value )\n",
    "def t_error( t ):\n",
    "    print(\"Invalid Token:\",t.value[0])\n",
    "    t.lexer.skip( 1 )\n",
    "\n",
    "lexer = lex.lex()\n",
    "precedence = (\n",
    "    ( 'left', 'PLUS', 'MINUS' ),\n",
    "    ( 'left', 'TIMES', 'DIV' ),\n",
    "    ( 'nonassoc', 'UMINUS' )\n",
    ")\n",
    "def p_add( p ) :\n",
    "    'expr : expr PLUS expr'\n",
    "    p[0] = p[1] + p[3]\n",
    "def p_sub( p ) :\n",
    "    'expr : expr MINUS expr'\n",
    "    p[0] = p[1] - p[3]\n",
    "def p_expr2uminus( p ) :\n",
    "    'expr : MINUS expr %prec UMINUS'\n",
    "    p[0] = - p[2]\n",
    "def p_mult_div( p ) :\n",
    "    '''expr : expr TIMES expr\n",
    "    | expr DIV expr'''\n",
    "    if p[2] == '*' :\n",
    "        p[0] = p[1] * p[3]\n",
    "    else :\n",
    "        if p[3] == 0 :\n",
    "            print(\"Can't divide by 0\")\n",
    "            raise ZeroDivisionError('integer division by 0')\n",
    "        p[0] = p[1] / p[3]\n",
    "def p_expr2NUM( p ) :\n",
    "    'expr : NUMBER'\n",
    "    p[0] = p[1]\n",
    "def p_parens( p ) :\n",
    "    'expr : LPAREN expr RPAREN'\n",
    "    p[0] = p[2]\n",
    "def p_error( p ):\n",
    "    print(\"Syntax error in input!\")\n",
    "    \n",
    "parser = yacc.yacc()\n",
    "res = parser.parse(\"-4*-(3-5)\") # the input\n",
    "print(res)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Windows [版本 10.0.16299.309]\n",
      "(c) 2017 Microsoft Corporation。保留所有权利。\n",
      "\n",
      "E:\\MyFile\\Jupyter\\Python-Learn\\Chapter 190 Python Lex-Yacc>python calc.py\n",
      "-8\n",
      "\n",
      "E:\\MyFile\\Jupyter\\Python-Learn\\Chapter 190 Python Lex-Yacc>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating LALR tables\n"
     ]
    }
   ],
   "source": [
    "%%cmd\n",
    "python calc.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 190.3: Part 1: Tokenizing Input with Lex"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "There are two steps that the code from example 1 carried out: one was tokenizing the input, which means it looked\n",
    "for symbols that constitute the arithmetic expression, and the second step was parsing, which involves analysing\n",
    "the extracted tokens and evaluating the result."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This section provides a simple example of how to tokenize user input, and then breaks it down line by line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "import ply.lex as lex\n",
    "# List of token names. This is always required\n",
    "tokens = [\n",
    "    'NUMBER',\n",
    "    'PLUS',\n",
    "    'MINUS',\n",
    "    'TIMES',\n",
    "    'DIVIDE',\n",
    "    'LPAREN',\n",
    "    'RPAREN',\n",
    "]\n",
    "# Regular expression rules for simple tokens\n",
    "t_PLUS = r'\\+'\n",
    "t_MINUS = r'-'\n",
    "t_TIMES = r'\\*'\n",
    "t_DIVIDE = r'/'\n",
    "t_LPAREN = r'\\('\n",
    "t_RPAREN = r'\\)'\n",
    "# A regular expression rule with some action code\n",
    "def t_NUMBER(t):\n",
    "    r'\\d+'\n",
    "    t.value = int(t.value)\n",
    "    return t\n",
    "# Define a rule so we can track line numbers\n",
    "def t_newline(t):\n",
    "    r'\\n+'\n",
    "    t.lexer.lineno += len(t.value)\n",
    "# A string containing ignored characters (spaces and tabs)\n",
    "t_ignore = ' \\t'\n",
    "# Error handling rule\n",
    "def t_error(t):\n",
    "    print(\"Illegal character '%s'\" % t.value[0])\n",
    "    t.lexer.skip(1)\n",
    "# Build the lexer\n",
    "lexer = lex.lex()\n",
    "# Give the lexer some input\n",
    "lexer.input(data)\n",
    "# Tokenize\n",
    "while True:\n",
    "    tok = lexer.token()\n",
    "    if not tok:\n",
    "        break # No more input\n",
    "    print(tok)\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Save this file as calclex.py . We'll be using this when building our Yacc parser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import the module using import ply.lex \n",
    "2. All lexers must provide a list called tokens that defines all of the possible token names that can be produced by the lexer. This list is always required.\n",
    "```Python\n",
    "    tokens = [\n",
    "        'NUMBER',\n",
    "        'PLUS',\n",
    "        'MINUS',\n",
    "        'TIMES',\n",
    "        'DIVIDE',\n",
    "        'LPAREN',\n",
    "        'RPAREN',\n",
    "    ]\n",
    "    ##tokens could also be a tuple of strings (rather than a string), where each string denotes a token as before.\n",
    "```\n",
    "3. The regex rule for each string may be defined either as a string or as a function. In either case, the variable\n",
    "name should be prefixed by `t_` to denote it is a rule for matching tokens.\n",
    "+ For simple tokens, the regular expression can be specified as strings: `t_PLUS = r'\\+'`\n",
    "+ If some kind of action needs to be performed, a token rule can be specified as a function.\n",
    "    ```Python\n",
    "    def t_NUMBER(t):\n",
    "        r'\\d+'\n",
    "        t.value = int(t.value)\n",
    "        return t\n",
    "    ```\n",
    "    Note, the rule is specified as a doc string within the function. The function accepts one argument which\n",
    "is an instance of LexToken , performs some action and then returns back the argument.\n",
    "    If you want to use an external string as the regex rule for the function instead of specifying a doc\n",
    "string, consider the following example:\n",
    "    ```Python\n",
    "    @TOKEN(identifier) # identifier is a string holding the regex\n",
    "    def t_ID(t):\n",
    "        ... # actions\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An instance of LexToken object (let's call this object t ) has the following attributes:\n",
    "1. t.type which is the token type (as a string) (eg: 'NUMBER' , 'PLUS' , etc). By default, t.type is set to the name following the t_ prefix.\n",
    "2. t.value which is the lexeme (the actual text matched) \n",
    "3. t.lineno which is the current line number (this is not automatically updated, as the lexer knows nothing of line numbers). Update lineno using a function called t_newline .\n",
    "```Python\n",
    "def t_newline(t):\n",
    "r'\\n+'\n",
    "t.lexer.lineno += len(t.value)\n",
    "```\n",
    "4. t.lexpos which is the position of the token relative to the beginning of the input text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If nothing is returned from a regex rule function, the token is discarded. If you want to discard a token, you can alternatively add t\\_ignore\\_ prefix to a regex rule variable instead of defining a function for the\n",
    "same rule.\n",
    "```Python\n",
    "def t_COMMENT(t):\n",
    "    r'\\#.*'\n",
    "    pass\n",
    "# No return value. Token discarded\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "...Is the same as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`t_ignore_COMMENT = r'\\#.*'`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This is of course invalid if you're carrying out some action when you see a comment. In which case, use\n",
    "a function to define the regex rule."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "If you haven't defined a token for some characters but still want to ignore it, use t_ignore =\n",
    "\"<characters to ignore>\" (these prefixes are necessary):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "t_ignore_COMMENT = r'\\#.*'\n",
    "t_ignore = ' \\t' # ignores spaces and tabs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When building the master regex, lex will add the regexes specified in the file as follows:\n",
    "\n",
    "    1. Tokens defined by functions are added in the same order as they appear in the file. \n",
    "    2. Tokens defined by strings are added in decreasing order of the string length of the string defining the regex for that token."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "If you are matching\n",
    "==\n",
    "and\n",
    "=\n",
    "in the same file, take advantage of these rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Literals are tokens that are returned as they are. Both t.type and t.value will be set to the character itself. Define a list of literals as such:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`literals = [ '+', '-', '*', '/' ]`\n",
    "or,\n",
    "`literals = \"+-*/\"`\n",
    "It is possible to write token functions that perform additional actions when literals are matched.\n",
    "However, you'll need to set the token type appropriately. For example:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "literals = [ '{', '}' ]\n",
    "def t_lbrace(t):\n",
    "    r'\\{'\n",
    "    t.type = '{' # Set token type to the expected literal (ABSOLUTE MUST if this is a literal)\n",
    "    return t\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Handle errors with t_error function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "# Error handling rule\n",
    "def t_error(t):\n",
    "    print(\"Illegal character '%s'\" % t.value[0])\n",
    "    t.lexer.skip(1) # skip the illegal token (don't process it)\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In general, t.lexer.skip(n) skips n characters in the input string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final preparations:**\n",
    "    \n",
    "Build the lexer using lexer = lex.lex() .\n",
    "\n",
    "You can also put everything inside a class and call use instance of the class to define the lexer. Eg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module '__main__' has no attribute '__file__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-c37b9684b939>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;31m# Build the lexer and try it out\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMyLexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Build the lexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"3 + 4\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-c37b9684b939>\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m# Build the lexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\songc\\Anaconda3\\lib\\site-packages\\ply\\lex.py\u001b[0m in \u001b[0;36mlex\u001b[1;34m(module, object, debug, optimize, lextab, reflags, nowarn, outputdir, debuglog, errorlog)\u001b[0m\n\u001b[0;32m    892\u001b[0m         \u001b[1;31m# If no __file__ attribute is available, try to obtain it from the __module__ instead\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'__file__'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mldict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 894\u001b[1;33m             \u001b[0mldict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'__file__'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mldict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'__module__'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    895\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    896\u001b[0m         \u001b[0mldict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_caller_module_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module '__main__' has no attribute '__file__'"
     ]
    }
   ],
   "source": [
    "import ply.lex as lex\n",
    "class MyLexer(object):\n",
    "    ... # everything relating to token rules and error handling comes here as usual\n",
    "    # List of token names. This is always required\n",
    "    tokens = [\n",
    "        'NUMBER',\n",
    "        'PLUS',\n",
    "        'MINUS',\n",
    "        'TIMES',\n",
    "        'DIVIDE',\n",
    "        'LPAREN',\n",
    "        'RPAREN',\n",
    "    ]\n",
    "    # Regular expression rules for simple tokens\n",
    "    t_PLUS = r'\\+'\n",
    "    t_MINUS = r'-'\n",
    "    t_TIMES = r'\\*'\n",
    "    t_DIVIDE = r'/'\n",
    "    t_LPAREN = r'\\('\n",
    "    t_RPAREN = r'\\)'\n",
    "    t_ignore = ' \\t'\n",
    "    # A regular expression rule with some action code\n",
    "    def t_NUMBER(t):\n",
    "        r'\\d+'\n",
    "        t.value = int(t.value)\n",
    "        return t\n",
    "    # Define a rule so we can track line numbers\n",
    "    def t_newline(t):\n",
    "        r'\\n+'\n",
    "        t.lexer.lineno += len(t.value)\n",
    "    # A string containing ignored characters (spaces and tabs)\n",
    "    \n",
    "    # Error handling rule\n",
    "    def t_error(t):\n",
    "        print(\"Illegal character '%s'\" % t.value[0])\n",
    "        t.lexer.skip(1)\n",
    "    # Build the lexer\n",
    "    def build(self, **kwargs):\n",
    "        self.lexer = lex.lex(module=self, **kwargs)\n",
    "    def test(self, data):\n",
    "        self.lexer.input(data)\n",
    "        for token in self.lexer.token():\n",
    "            print(token)\n",
    "    # Build the lexer and try it out\n",
    "m = MyLexer()\n",
    "m.build() # Build the lexer\n",
    "m.test(\"3 + 4\") "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Provide input using lexer.input(data) where data is a string"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To get the tokens, use lexer.token() which returns tokens matched. You can iterate over lexer in a loop as in:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "for i in lexer:\n",
    "    print(i)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 190.4: Part 2: Parsing Tokenized Input with Yacc"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This section explains how the tokenized input from Part 1 is processed - it is done using Context Free Grammars (CFGs). The grammar must be specified, and the tokens are processed according to the grammar. Under the hood, the parser uses an LALR parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LexToken(NUMBER,10,1,0)\n",
      "LexToken(PLUS,'+',1,3)\n",
      "LexToken(NUMBER,5,1,5)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'__file__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-df1b31594fa4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Syntax error in input!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m# Build the parser\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0myacc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myacc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\songc\\Anaconda3\\lib\\site-packages\\ply\\yacc.py\u001b[0m in \u001b[0;36myacc\u001b[1;34m(method, debug, module, tabmodule, start, check_recursion, optimize, write_tables, debugfile, outputdir, debuglog, errorlog, picklefile)\u001b[0m\n\u001b[0;32m   3248\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3249\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;34m'.'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtabmodule\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3250\u001b[1;33m                 \u001b[0msrcfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpdict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'__file__'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3251\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3252\u001b[0m                 \u001b[0mparts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtabmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '__file__'"
     ]
    }
   ],
   "source": [
    "# Yacc example\n",
    "import ply.yacc as yacc\n",
    "# Get the token map from the lexer. This is required.\n",
    "from calclex import tokens\n",
    "def p_expression_plus(p):\n",
    "    'expression : expression PLUS term'\n",
    "    p[0] = p[1] + p[3]\n",
    "def p_expression_minus(p):\n",
    "    'expression : expression MINUS term'\n",
    "    p[0] = p[1] - p[3]\n",
    "def p_expression_term(p):\n",
    "    'expression : term'\n",
    "    p[0] = p[1]\n",
    "def p_term_times(p):\n",
    "    'term : term TIMES factor'\n",
    "    p[0] = p[1] * p[3]\n",
    "def p_term_div(p):\n",
    "    'term : term DIVIDE factor'\n",
    "    p[0] = p[1] / p[3]\n",
    "def p_term_factor(p):\n",
    "    'term : factor'\n",
    "    p[0] = p[1]\n",
    "def p_factor_num(p):\n",
    "    'factor : NUMBER'\n",
    "    p[0] = p[1]\n",
    "def p_factor_expr(p):\n",
    "    'factor : LPAREN expression RPAREN'\n",
    "    p[0] = p[2]\n",
    "# Error rule for syntax errors\n",
    "def p_error(p):\n",
    "    print(\"Syntax error in input!\")\n",
    "# Build the parser\n",
    "parser = yacc.yacc()\n",
    "while True:\n",
    "    try:\n",
    "        s = raw_input('calc > ')\n",
    "    except EOFError:\n",
    "        break\n",
    "    if not s: continue\n",
    "    result = parser.parse(s)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Breakdown**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Each grammar rule is defined by a function where the docstring to that function contains the appropriate context-free grammar specification. The statements that make up the function body implement the semantic actions of the rule. Each function accepts a single argument p that is a sequence containing the values of\n",
    "each grammar symbol in the corresponding rule. The values of p[i] are mapped to grammar symbols as shown here:\n",
    "```Python\n",
    "def p_expression_plus(p):\n",
    "    'expression : expression PLUS term'\n",
    "    # ^ ^ ^ ^\n",
    "    # p[0] p[1] p[2] p[3]\n",
    "    p[0] = p[1] + p[3]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ For tokens, the \"value\" of the corresponding p[i] is the same as the p.value attribute assigned in the lexer\n",
    "module. So, PLUS will have the value + ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ For non-terminals, the value is determined by whatever is placed in p[0] . If nothing is placed, the value is\n",
    "None. Also, p[-1] is not the same as p[3] , since p is not a simple list ( p[-1] can specify embedded actions\n",
    "(not discussed here))."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Note that the function can have any name, as long as it is preceeded by p_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The p_error(p) rule is defined to catch syntax errors (same as yyerror in yacc/bison)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Multiple grammar rules can be combined into a single function, which is a good idea if productions have a\n",
    "similar structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "def p_binary_operators(p):\n",
    "    '''expression : expression PLUS term\n",
    "    | expression MINUS term\n",
    "    term : term TIMES factor\n",
    "    | term DIVIDE factor'''\n",
    "    if p[2] == '+':\n",
    "        p[0] = p[1] + p[3]\n",
    "    elif p[2] == '-':\n",
    "        p[0] = p[1] - p[3]\n",
    "    elif p[2] == '*':\n",
    "        p[0] = p[1] * p[3]\n",
    "    elif p[2] == '/':\n",
    "        p[0] = p[1] / p[3]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Character literals can be used instead of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "def p_binary_operators(p):\n",
    "    '''expression : expression '+' term\n",
    "    | expression '-' term\n",
    "    term : term '*' factor\n",
    "    | term '/' factor'''\n",
    "    if p[2] == '+':\n",
    "        p[0] = p[1] + p[3]\n",
    "    elif p[2] == '-':\n",
    "        p[0] = p[1] - p[3]\n",
    "    elif p[2] == '*':\n",
    "        p[0] = p[1] * p[3]\n",
    "    elif p[2] == '/':\n",
    "        p[0] = p[1] / p[3]\n",
    "```\n",
    "Of course, the literals must be specified in the lexer module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Empty productions have the form '''symbol : '''\n",
    "+ To explicitly set the start symbol, use start = 'foo' , where foo is some non-terminal.\n",
    "+ Setting precedence and associativity can be done using the precedence variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "precedence = (\n",
    "    ('nonassoc', 'LESSTHAN', 'GREATERTHAN'), # Nonassociative operators\n",
    "    ('left', 'PLUS', 'MINUS'),\n",
    "    ('left', 'TIMES', 'DIVIDE'),\n",
    "    ('right', 'UMINUS'), # Unary minus operator\n",
    ")\n",
    "```\n",
    "Tokens are ordered from lowest to highest precedence. nonassoc means that those tokens do not associate. This means that something like a < b < c is illegal whereas a < b is still legal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ parser.out is a debugging file that is created when the yacc program is executed for the first time. Whenever a shift/reduce conflict occurs, the parser always shifts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
